{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eye_detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE3Ld7PmAI0-",
        "outputId": "47e4870f-e530-4b35-af02-0881f56c2bd6"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "# GOTO edit, notebook settings, GPU. \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypSY_XTeAquJ"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import tensorflow as tf           # Training and inference of deep neural networks\n",
        "import cv2                        # Computer Vision\n",
        "import matplotlib.pyplot as plt   # Visualizations in Python\n",
        "import numpy as np                # Calculating stuff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rryxYvnNWoHY"
      },
      "source": [
        "### Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIKKAmjdPwDq"
      },
      "source": [
        "!mkdir database\n",
        "!cd database\n",
        "\n",
        "# Upload zip of dataset in google drive and make it open for everyone. Copy link\n",
        "# Link looks like this: drive.google.com/file/d/someAlphaNumeric/view?usp=sharing, the someAlphaNumeric is your ID.\n",
        "\n",
        "!gdown --id 1d7JR3352rqy-f6-fr6HGny7aNcMuXI2J\n",
        "\n",
        "# Will be downloaded to working directory.\n",
        "\n",
        "!unzip eye_database_4000x2.zip\n",
        "\n",
        "# If not doing in colab then the method of uploading is left to your discretion."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjWJkgtYx7_Z"
      },
      "source": [
        "!ls \n",
        "training_data =[]                                                                      # Initialize list for dataset.\n",
        "classes = [\"closed\", \"open\"]                                                           # 2 Classes; classes[0] = closed_eyes, classes[1] = open_eyes\n",
        "datadirectory = r\".\"                                                                   # Directory of dataset, depends on how the user applies this. \n",
        "img_size = 224                                                                         # Let img_size that we defined be 224 pixels\n",
        "def create_training_data():\n",
        "    for category in classes:                                                           # For each category:\n",
        "        path=os.path.join(datadirectory, category) \n",
        "        print(path)                                                                    # Prints path of file\n",
        "        class_num= classes.index(category)\n",
        "        for img in os.listdir(path):\n",
        "            print(img)\n",
        "            try:\n",
        "                img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)   # Read image in GREYSCALE\n",
        "                backtorgb = cv2.cvtColor(img_array,cv2.COLOR_GRAY2RGB)                 # Convert GRAY to RGB\n",
        "                new_array = cv2.resize(backtorgb, (img_size,img_size))                 # Resize image to the img_size we set\n",
        "                training_data.append((new_array, class_num))                           # Append new array and type (closed or open) to list of dataset\n",
        "            except Exception as e:\n",
        "                pass\n",
        "create_training_data()                                                                 # run create_training_data() function.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXaayDNE0wsw"
      },
      "source": [
        "print(len(training_data)) # just checking xD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlVdf7B0WjTb"
      },
      "source": [
        "### Shuffling Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erOsUNEX1xhl"
      },
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(training_data)                                                          # Randomizing training_data list to prevent bias (if any)\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for features,label in training_data:                                                   # Create new lists \n",
        "    x.append(features)                                                                 # x has image data\n",
        "    y.append(label)                                                                    # y has label (open or closed) \n",
        "\n",
        "x = np.array(x).reshape(-1, img_size, img_size, 3 )                                    # np.array() converts argument into Class 'numpy.ndarray'\n",
        "                                                                                       # reshape ndarray object into array of dimension of (-1, 224, 224, 3) (yes 4 dimensions here)\n",
        "                                                                                       # Dimensions are counted in direction: from outside to inside. \n",
        "                                                                                       # [ [ [ [a, b, c], [d, e, f], ...244 times ], [ [a, b, c], [d, e, f], ...244 times ], ...244 times ] ...len(x) times]\n",
        "                                                                                       # In this case dimension of -1 is equivalant to len(x).                                                                                     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIJ9tXPU1-dL"
      },
      "source": [
        "x.shape                                                                                # An attribute of ndarray. Prints tuple with each index having the number of elements in each dimension\n",
        "x= x/255.0                                                                             # Normalizing each value with respect to it having 256 pixels\n",
        "Y=np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaWSUWra_1Wi"
      },
      "source": [
        "At `denomenator = (256-1)`, `x= x/255` might be too heavy.  \n",
        "Try `denomenator = [(256/16)-1]` and keep going on to 32, 64...  \n",
        "Then try using more datasets to get better accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIRdqKTrA2iK"
      },
      "source": [
        "### Pickling Data  \n",
        "Pickle converts any kind of python objects (list, dict, etc.) into byte stream (0s and 1s). This is called pickling or serialization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrvNLF9U19GS"
      },
      "source": [
        "import pickle \n",
        "pickle_out= open(\"x.pickle\",\"wb\")                                                     # Open pickle instance to write(w) in it in binary(b)\n",
        "pickle.dump(x,pickle_out)                                                             # Dump list x into pickle\n",
        "pickle_out.close()                                                                    # Create and save pickle instance \n",
        "\n",
        "pickle_out= open(\"y.pickle\",\"wb\")\n",
        "pickle.dump(y,pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "\n",
        "pickle_in = open(\"x.pickle\",\"rb\")                                                     # Open pickle instance to read(r) in it in binary(b)\n",
        "x = pickle.load(pickle_in)                                                            # Load information into variable x\n",
        "\n",
        "pickle_in = open(\"y.pickle\",\"rb\")\n",
        "y = pickle.load(pickle_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEh5QJ8-WVUw"
      },
      "source": [
        "### CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwf4vZiE2U73"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "# Yes it was imported again."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIGEPTDt2XO8"
      },
      "source": [
        "model= tf.keras.applications.mobilenet.MobileNet()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO9T1auoD5UA"
      },
      "source": [
        "Mobilenet is an architecture that uses depthwise separable convolutions to construct lightweight deep convolutional neural networks and provides an efficient model for mobile and embedded vision applications. It has fewer parameters and higher classification accuracy as compared to other architectures.  \n",
        "By default, mobilenet uses softmax classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLVorf-UWNUR"
      },
      "source": [
        "### Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOdMrlxJVFjO"
      },
      "source": [
        "base_input= model.layers[0].input\n",
        "base_output = model.layers[-4].output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGdQNQBIWy-W"
      },
      "source": [
        "Flat_layer= layers.Flatten()(base_output)\n",
        "final_output = layers.Dense(1)(Flat_layer)\n",
        "final_output= layers.Activation('sigmoid')(final_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUngcvmrW1g3"
      },
      "source": [
        "new_model = keras.Model(inputs = base_input, outputs= final_output)\n",
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvyzfulTW8Zt"
      },
      "source": [
        "new_model.compile(loss=\"binary_crossentropy\", optimizer = \"adam\", metrics=[\"accuracy\"])\n",
        "new_model.fit(x,Y, epochs = 1,validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySFyTZ6LXBI9"
      },
      "source": [
        "new_model.save('my_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09yxisxnXDtw"
      },
      "source": [
        "new_model = tf.keras.models.load_model('my_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRHvrPGLXEpc"
      },
      "source": [
        "### Checking Network for prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6pSeQrEXHLO"
      },
      "source": [
        "img_array = cv2.imread(r'path/to/random_img', cv2.IMREAD_GRAYSCALE)\n",
        "backtorgb = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)\n",
        "new_array =cv2.resize(backtorgb, (img_size,img_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjxCgu-bXbZY"
      },
      "source": [
        "x_input = np.array(new_array).reshape(1, img_size, img_size, 3)\n",
        "x_input.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AaGAblnXefs"
      },
      "source": [
        "plt.imshow(new_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSwQoctNXhSL"
      },
      "source": [
        "x_input=x_input/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA-kwEerXlm2"
      },
      "source": [
        "prediction = new_model.predict(x_input)\n",
        "prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2Fz_nemX21U"
      },
      "source": [
        "## Predicting in unknown images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkJIsB-MXqX_"
      },
      "source": [
        "img = cv2.imread(r'path/to/unknown_img')\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmpz0bN5YGzi"
      },
      "source": [
        "faceCascade=cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3IqSQ3OYTyw"
      },
      "source": [
        "eyes=eye_Cascade.detectMultiScale(gray,1.1,4)\n",
        "for(x, y, w, h) in eyes[:]:\n",
        "    cv2.rectangle(img, (x,y),(x+w, y+h), (0,255,0),2)\n",
        "print(eyes)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPrNdTftYidA"
      },
      "source": [
        "eye_Cascade=cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_eye.xml')\n",
        "gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "eyes=eye_Cascade.detectMultiScale(gray,1.1,4)\n",
        "# To identify eyes in unknown image\n",
        "for(x, y, w, h) in eyes:\n",
        "    roi_gray=gray[y:y+h, x:x+w]\n",
        "    roi_color=img[y:y+h, x:x+w]\n",
        "    eyess=eye_Cascade.detectMultiScale(roi_gray)\n",
        "    if len(eyes) == 0:\n",
        "            print(\"eyes are not detected\")\n",
        "    else:\n",
        "        for(ex, ey, ew, eh) in eyess:\n",
        "            eyes_roi = roi_color[ey: ey + eh, ex: ex + ew]\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) # Displays rectangle over detected eyes "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLmrAicuZKw-"
      },
      "source": [
        "to_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTOFQjt8ZMqM"
      },
      "source": [
        "plt.imshow(cv2.cvtColor(eyes_roi, cv2.COLOR_BGR2RGB)) # shows one identified eye"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o11jY56tZV0z"
      },
      "source": [
        "eyes_roi.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_Y4lrGOiVVa"
      },
      "source": [
        "final_image=cv2.resize(eyes_roi,(224, 224))\n",
        "final_image=np.expand_dims(final_image,axis=0)\n",
        "final_image=final_image/225.0\n",
        "\n",
        "final_image.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh14kqw9idlm"
      },
      "source": [
        "new_model.predict(final_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcDVxLmjifOP"
      },
      "source": [
        "## Applying to Webcam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds1VAMQYiem2"
      },
      "source": [
        "import cv2 \n",
        "\n",
        "path  = \"haarcascade frontalface_default.xml\" \n",
        "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades+ 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# Check if the webcam is opened correctly\n",
        "\n",
        "if not cap.isOpened(): \n",
        "    cap = cv2.VideoCapture(1)\n",
        "if not cap.isOpened(): \n",
        "    raise I0Error(\"Cannot open webcam\")\n",
        "\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
        "    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    eyes = eye_Cascade.detectMultiScale(gray, 1.1, 4)\n",
        "\n",
        "    for x,y,w,h in eyes:\n",
        "        roi_gray = gray[y:y+h, x:x+w] \n",
        "        roi_color = frame[y:y+h, x:x+w]\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2) \n",
        "        eyess = eye_Cascade.detectMultiScale(roi_gray) \n",
        "        if len(eyes) == 0:\n",
        "            print(\"Eyes not detected\")\n",
        "        else:\n",
        "            for (ex,ey,ew,eh) in eyess: \n",
        "                eyes_roi = roi_color[ey: ey + eh, ex:ex + ew]\n",
        "                \n",
        "        final_image=cv2.resize(eyes_roi,(224, 224))\n",
        "        final_image=np.expand_dims(final_image,axis=0)\n",
        "        final_image=final_image/225.0\n",
        "\n",
        "\n",
        "\n",
        "    Predictions = new_model.predict(final_image) \n",
        "    if (Predictions > 0):\n",
        "        status = \"Open_Eyes\"\n",
        "    else:\n",
        "        status = \"Closed Eyes\"\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    print(faceCascade.empty())\n",
        "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
        "    \n",
        "    # To draw a rectangle around features\n",
        "    for (x, y, w, h) in faces:\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "    # Using putText() methhod for inserting text in video\n",
        "    \n",
        "    cv2.putText(frame, status, (50, 50), font, 3, (0, 0, 255), 2, cv2.LINE_4)\n",
        "    cv2.imshow(\"Drowsiness Detected\", frame)\n",
        "    \n",
        "    if cv2.waitKey(2) & 0xFF == ord('q'):\n",
        "        break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}